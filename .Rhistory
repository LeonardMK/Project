list_settings <- list(
`DML algorithm` = dml_est$dml_procedure,
`N Folds` = dml_est$n_folds,
`N Rep` = dml_est$n_rep,
`Learner` = dml_est$learner,
`Tuning Result` = dml_est$tuning_res,
`Sample Splits` = dml_est$smpls
)
list_predictions <- dml_est$predictions
names(list_predictions) <- str_remove(names(list_predictions), "ml_")
list(
Estimates = vec_results,
Settings = list_settings,
Predictions = list_predictions,
Tuning_Results = dt_tuning_results,
Time = dbl_time_taken
)
})
names(list_tuning) <- names(par_grids)
# Find best learner
dt_tune_result <- list_tuning %>%
map_dfr(~ {
.x$Tuning_Results %>%
select(
tune_settings$measure$ml_g,
tune_settings$measure$ml_m,
fun,
rep,
fold,
mle,
learner_param_vals
)
})
str_msr_g <- tune_settings$measure[["ml_g"]]
str_msr_m <- tune_settings$measure[["ml_m"]]
df_msrs <- dt_tune_result %>%
mutate(
msr = case_when(
fun == "ml_g" ~ !!sym(str_msr_g),
fun == "ml_m" ~ !!sym(str_msr_m)
)
) %>%
group_by(fun, mle) %>%
summarise(
mean_msr = mean(msr)
) %>%
mutate(
min_msr = case_when(
fun == "ml_g" & mean_msr == min(mean_msr) ~ TRUE,
fun == "ml_m" & mean_msr == min(mean_msr) ~ TRUE,
TRUE ~ FALSE
)
# One dataframe containing results
df_estimates <- list_tuning %>%
map(~{
vec_results <- c(
.x$Estimates,
"ml_g" = .x$Settings$Learner$ml_g$id,
"ml_m" = .x$Settings$Learner$ml_m$id
)
}) %>%
map_dfr( ~ .x)
# A list containing lists with specifications and predictions
list_settings_all <- list_tuning %>%
map(~ .x %>% pluck("Settings")) %>%
set_names(
c(
paste0("g: ", ml_g, " m: ", ml_m)
)
# List containing predictions
list_predictions_all <- list_tuning %>%
map(~ .x %>% pluck("Predictions")) %>%
set_names(
c(
paste0("g: ", ml_g, " m: ", ml_m)
)
# Find the best ml estimators
str_g <- df_msrs %>% filter(fun == "ml_g", min_msr) %>% pull(mle)
str_m <- df_msrs %>% filter(fun == "ml_m", min_msr) %>% pull(mle)
str_name_best <- paste0("g best: ", str_g, " m best: ", str_m, collapse = "")
if (which(grid_lrns$str_g == str_g) != which(grid_lrns$str_m == str_m)) {
# Create a new dml object with said algorithms and the given specifications
# SVM need an explicit mention what kind of classif/regr will be performed
if(str_detect(str_g, "\\.svm$")) {
lrn_g = exec(lrn, !!!str_g, type = if_else(str_type_y == "regr", "eps-regression", "C-classification"))
} else {
lrn_g = exec(lrn, !!!str_g)
}
if(str_detect(str_m, "\\.svm$")) {
lrn_m = exec(lrn, !!!str_m, type = if_else(str_type_d == "regr", "eps-regression", "C-classification"))
} else {
lrn_m = exec(lrn, !!!str_m)
}
dml_est <- dml_class$new(
data_dml,
lrn_g,
lrn_m,
...,
draw_sample_splitting = draw_sample_splitting
)
if (!draw_sample_splitting) dml_est$set_sample_splitting(list_samples)
# Get optimal parameters
list_params_ml_g <- list_tuning %>%
pluck(str_remove(str_g, "^.*\\.")) %>%
pluck("Settings") %>%
pluck("Tuning Result") %>%
pluck(d_cols) %>%
map(~ .x$ml_g$params)
list_params_ml_m <- list_tuning %>%
pluck(str_remove(str_m, "^.*\\.")) %>%
pluck("Settings") %>%
pluck("Tuning Result") %>%
pluck(d_cols) %>%
map(~ .x$ml_m$params)
dml_est$set_ml_nuisance_params("ml_g", d_cols, list_params_ml_g, set_fold_specific = TRUE)
dml_est$set_ml_nuisance_params("ml_m", d_cols, list_params_ml_m, set_fold_specific = TRUE)
ddpcr::quiet(dml_est$fit(store_predictions = TRUE))
vec_results <- c(dml_est$coef, dml_est$se, DF = NA, dml_est$pval, dml_est$learner$ml_g$id, dml_est$learner$ml_m$id)
names(vec_results) <- c("parameter_est", "sd", "df", "p_value", "ml_g", "ml_m")
list_settings <- list(
`DML algorithm` = dml_est$dml_procedure,
`N Folds` = dml_est$n_folds,
`N Rep` = dml_est$n_rep,
`Learner` = dml_est$learner,
`Tuning Result` = dml_est$tuning_res,
`Sample Splits` = dml_est$smpls
)
list_predictions <- dml_est$predictions
names(list_predictions) <- str_remove(names(list_predictions), "ml_")
# Now append the results from the best model
df_estimates <- df_estimates %>%
rbind(vec_results)
list_settings_all <- list_settings_all %>%
append(
list(
list_settings
)
list_predictions_all <- list_predictions_all %>%
append(
list(
list_predictions
)
names(list_settings_all)[length(list_settings_all)] <- str_name_best
names(list_predictions_all)[length(list_predictions_all)] <- str_name_best
} else {
# Just Rename the elements that are best
str_best_pattern <- paste0("^g: ", str_g, " m: ", str_m, "$")
names(list_settings_all)[str_detect(names(list_settings_all), str_best_pattern)] <- str_name_best
names(list_predictions_all)[str_detect(names(list_predictions_all), str_best_pattern)] <- str_name_best
}
# Return a list
list(
Estimates = df_estimates,
Settings = list_settings_all,
Predictions = list_predictions_all,
Measrues = df_msrs
)
}
dml_estimator(test_dataset, x_cols = paste0("X.", 1:2), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
draw_sample_splitting = FALSE, tune = FALSE,
# tune_settings = list_tune_settings,
# par_grids = list_parameterspace
)
test_dataset <- test$datasets$`Sample = 100 with N = 250`$data
dml_estimator(test_dataset, x_cols = paste0("X.", 1:2), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
draw_sample_splitting = FALSE, tune = FALSE,
# tune_settings = list_tune_settings,
# par_grids = list_parameterspace
)
vec_ml_g <- c("regr.glmnet", "regr.xgboost", "regr.ranger", "regr.rpart", "regr.kknn")
vec_ml_m <- c("classif.glmnet", "classif.xgboost", "classif.ranger", "classif.rpart", "classif.kknn")
test_dataset <- test$datasets$`Sample = 100 with N = 250`$data
dml_estimator(test_dataset, x_cols = paste0("X.", 1:2), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
draw_sample_splitting = FALSE, tune = FALSE,
# tune_settings = list_tune_settings,
# par_grids = list_parameterspace
)
dml_estimator(test_dataset, x_cols = paste0("X.", 1:2), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
draw_sample_splitting = FALSE, tune = FALSE,
# tune_settings = list_tune_settings,
# par_grids = list_parameterspace
)
names(par_grids)
list_tuning
list_tuning[[1]]
list_tuning %>% length()
names(list_tuning) <- names(par_grids)
names(par_grids)
par_grids <- par_grids[c(1:4, 6)]
names(list_tuning) <- names(par_grids)
dt_tune_result <- list_tuning %>%
map_dfr(~ {
.x$Tuning_Results %>%
select(
tune_settings$measure$ml_g,
tune_settings$measure$ml_m,
fun,
rep,
fold,
mle,
learner_param_vals
)
})
list_tuning %>% map_dfr(~ {
.x$Tuning_Results %>%
select(
tune_settings$measure$ml_g,
tune_settings$measure$ml_m,
fun,
rep,
fold,
mle,
learner_param_vals
)
})
list_tuning$glmnet$Tuning_Results
list_tuning %>% map_dfr(~ {.x$Tuning_Results})
list_tuning$glmnet$Tuning_Results
list_tuning %>% map_dfr(~ {
.x$Tuning_Results %>%
select(
tune_settings$measure$ml_g,
tune_settings$measure$ml_m,
fun,
rep,
fold,
mle
)
})
x <- list_tuning$glmnet$Tuning_Results
tune_settings$measure$ml_g
tune_settings$measure$ml_g$fun()
tune_settings$measure$ml_g$id
# Find best learner
dt_tune_result <- list_tuning %>%
map_dfr(~ {
.x$Tuning_Results %>%
select(
tune_settings$measure$ml_g$id,
tune_settings$measure$ml_m$id,
fun,
rep,
fold,
mle,
learner_param_vals
)
})
dt_tune_result
dt_tune_result %>% View()
str_msr_g <- tune_settings$measure[["ml_g"]]
str_msr_m <- tune_settings$measure[["ml_m"]]
df_msrs <- dt_tune_result %>%
mutate(
msr = case_when(
fun == "ml_g" ~ !!sym(str_msr_g),
fun == "ml_m" ~ !!sym(str_msr_m)
)
) %>%
group_by(fun, mle) %>%
summarise(
mean_msr = mean(msr)
) %>%
mutate(
min_msr = case_when(
fun == "ml_g" & mean_msr == min(mean_msr) ~ TRUE,
fun == "ml_m" & mean_msr == min(mean_msr) ~ TRUE,
TRUE ~ FALSE
)
dt_tune_result %>%
mutate(
msr = case_when(
fun == "ml_g" ~ !!sym(str_msr_g),
fun == "ml_m" ~ !!sym(str_msr_m)
)
str_msr_g
str_msr_g <- tune_settings$measure[["ml_g"]]$id
str_msr_m <- tune_settings$measure[["ml_m"]]$id
df_msrs <- dt_tune_result %>%
mutate(
msr = case_when(
fun == "ml_g" ~ !!sym(str_msr_g),
fun == "ml_m" ~ !!sym(str_msr_m)
)
) %>%
group_by(fun, mle) %>%
summarise(
mean_msr = mean(msr)
) %>%
mutate(
min_msr = case_when(
fun == "ml_g" & mean_msr == min(mean_msr) ~ TRUE,
fun == "ml_m" & mean_msr == min(mean_msr) ~ TRUE,
TRUE ~ FALSE
)
df_msrs
# One dataframe containing results
df_estimates <- list_tuning %>%
map(~{
vec_results <- c(
.x$Estimates,
"ml_g" = .x$Settings$Learner$ml_g$id,
"ml_m" = .x$Settings$Learner$ml_m$id
)
}) %>%
map_dfr( ~ .x)
# A list containing lists with specifications and predictions
list_settings_all <- list_tuning %>%
map(~ .x %>% pluck("Settings")) %>%
set_names(
c(
paste0("g: ", ml_g, " m: ", ml_m)
)
# List containing predictions
list_predictions_all <- list_tuning %>%
map(~ .x %>% pluck("Predictions")) %>%
set_names(
c(
paste0("g: ", ml_g, " m: ", ml_m)
)
# Find the best ml estimators
str_g <- df_msrs %>% filter(fun == "ml_g", min_msr) %>% pull(mle)
str_m <- df_msrs %>% filter(fun == "ml_m", min_msr) %>% pull(mle)
str_g
str_m
which(grid_lrns$str_g == str_g) != which(grid_lrns$str_m == str_m)
# Just Rename the elements that are best
str_best_pattern <- paste0("^g: ", str_g, " m: ", str_m, "$")
names(list_settings_all)[str_detect(names(list_settings_all), str_best_pattern)] <- str_name_best
str_name_best <- paste0("g best: ", str_g, " m best: ", str_m, collapse = "")
str_name_best
str_best_pattern
# Just Rename the elements that are best
str_best_pattern <- paste0("g: ", str_g, " m: ", str_m, "$")
names(list_settings_all)[str_detect(names(list_settings_all), str_best_pattern)] <- str_name_best
names(list_predictions_all)[str_detect(names(list_predictions_all), str_best_pattern)] <- str_name_best
# Return a list
list(
Estimates = df_estimates,
Settings = list_settings_all,
Predictions = list_predictions_all,
Measrues = df_msrs
)
df_estimates
remotes::install_github("mlr3learners/mlr3learners.nnet")
install.packages("remote")
remotes::install_github("mlr3learners/mlr3learners.nnet")
install.packages("remotes")
remotes::install_github("mlr3learners/mlr3learners.nnet")
lrns()
install.packages("mlr3extralearners")
remotes::install_github("mlr-org/mlr3extralearners")
library(mlr3verse)
remotes::install_github("mlr-org/mlr3extralearners")
library(mlr3extralearners)
# Neural Networks for Regression
create_learner(
pkg = ".",
classname = "nnet",
algorithm = "Single Layer Neural Network",
type = "regr",
key = "nnet",
package = "nnet",
caller = "nnet",
feature_types = c("logical", "integer", "numeric", "factor", "ordered"),
predict_types = "response"
)
# Neural Networks for Regression
create_learner(
pkg = "C:/Users/Wilms/Documents/R/win-library/4.1/",
classname = "nnet",
algorithm = "Single Layer Neural Network",
type = "regr",
key = "nnet",
package = "nnet",
caller = "nnet",
feature_types = c("logical", "integer", "numeric", "factor", "ordered"),
predict_types = "response"
)
# Neural Networks for Regression
create_learner(
pkg = "C:/Users/Wilms/Documents/R/win-library/4.1/mlr3extralearners/",
classname = "nnet",
algorithm = "Single Layer Neural Network",
type = "regr",
key = "nnet",
package = "nnet",
caller = "nnet",
feature_types = c("logical", "integer", "numeric", "factor", "ordered"),
predict_types = "response"
)
# Neural Networks for Regression
create_learner(
pkg = "C:/Users/Wilms/Documents/R/win-library/4.1/mlr3extralearners/",
classname = "nnet",
algorithm = "Single Layer Neural Network",
type = "regr",
key = "nnet",
package = "nnet",
caller = "nnet",
feature_types = c("logical", "integer", "numeric", "factor", "ordered"),
predict_types = "response",
references = TRUE
gh_name = "LeonardMK"
)
# Neural Networks for Regression
create_learner(
pkg = "C:/Users/Wilms/Documents/R/win-library/4.1/mlr3extralearners/",
classname = "nnet",
algorithm = "Single Layer Neural Network",
type = "regr",
key = "nnet",
package = "nnet",
caller = "nnet",
feature_types = c("logical", "integer", "numeric", "factor", "ordered"),
predict_types = "response",
references = TRUE,
gh_name = "LeonardMK"
)
# Neural Networks for Regression
create_learner(
pkg = "C:/Users/Wilms/Documents/R/win-library/4.1/mlr3extralearners/",
classname = "nnet",
algorithm = "Single Layer Neural Network",
type = "regr",
key = "Nnet",
package = "Nnet",
caller = "Nnet",
feature_types = c("logical", "integer", "numeric", "factor", "ordered"),
predict_types = "response",
references = TRUE,
gh_name = "LeonardMK"
)
library(mlr3)
library(paradox)
library(data.table)
test_data <- test$datasets$`Sample = 1 with N = 100`$data %>%
mutate(D = as.factor(D))
task_regr <- TaskRegr$new("y_col", test_data, target = "Y")
task_classif <- TaskClassif$new("d_col", test_data, target = "D")
# Now create a dml model
list_tune_settings <- list(
terminator = trm("evals", n_evals = 5),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = "regr.mse", ml_m = "classif.logloss")
)
dml_data <- DoubleMLData$new(
test_data,
y_col = "Y",
x_cols = c("X.1", "X.2"),
d_cols = "D"
)
dml_est <- DoubleMLPLR$new(
data = dml_data,
ml_g = lrn("regr.svm", type = "eps-regression"),
ml_m = lrn("classif.svm", type = "C-classification"),
n_folds = 3,
n_rep = 1
)
dml_est$tune(
param_set = list_svm,
tune_settings = list_tune_settings,
tune_on_folds = TRUE
)
library(plotly)
# What functions to create
# Sparse but high dimensional. No interactions. Maybe even linear
dgp_sparse <- function(X, E = rnorm(nrow(X)), beta = rep(1, ncol(X))){
X %*% beta + E
}
# Setting up data for plotting
vec_beta <- rnorm(2, sd = 10)
vec_mu <- runif(2, min = -10, max = 10)
mat_Sigma <- diag(runif(2, min = 1, max = 10))
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma)
X %>% dgp_sparse(beta = vec_beta)
X <- MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma)
# Setting up data for plotting
int_N <- 1000
X <- MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma)
X %>% dgp_sparse(beta = vec_beta)
X <- MASS::mvrnorm(int_N, mu = vec_mu, Sigma = mat_Sigma)
X %>% dgp_sparse(beta = vec_beta)
X <- MASS::mvrnorm(int_N, mu = vec_mu, Sigma = mat_Sigma) %>%
as.data.frame()
X %>%
mutate(Y_sparse = dgp_sparse(beta = vec_beta))
df_sim <- MASS::mvrnorm(int_N, mu = vec_mu, Sigma = mat_Sigma) %>%
as.data.frame()
df_sim$Y_sparse <- dgp_sparse(df_sim[, 1:2], vec_beta)
df_sim <- MASS::mvrnorm(int_N, mu = vec_mu, Sigma = mat_Sigma) %>%
as.data.frame()
X <- MASS::mvrnorm(int_N, mu = vec_mu, Sigma = mat_Sigma)
# What functions to create
# Sparse but high dimensional. No interactions. Maybe even linear
dgp_sparse <- function(X, E = rnorm(nrow(X)), beta = rep(1, ncol(X))){
X %*% beta + E
}
df_sim$Y_sparse <- dgp_sparse(X, beta = vec_beta)
plot_ly(x = X[, 1], y = X[, 2], z = df_sim$Y_sparse)
plot_ly(x = X[, 1], y = X[, 2], z = df_sim$Y_sparse) %>%
add_surface()
plot_ly(x = X[, 1], y = X[, 2], z = df_sim$Y_sparse) %>%
add_surface()
