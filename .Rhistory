)
# Find the best ml estimators
# In case no cross fitting was performed choose learner with min in sample.
if (rsmp_key == "no_cf"){
str_g <- df_msrs %>% filter(fun == "ml_g", min_msr_in) %>% pull(mle)
str_m <- df_msrs %>% filter(fun == "ml_m", min_msr_in) %>% pull(mle)
} else {
str_g <- df_msrs %>% filter(fun == "ml_g", min_msr_val) %>% pull(mle)
str_m <- df_msrs %>% filter(fun == "ml_m", min_msr_val) %>% pull(mle)
}
str_name_best <- paste0("g best: ", str_g, " m best: ", str_m, collapse = "")
# Refitting DML with Best Learners ----------------------------------------
if (which(grid_lrns$str_g == str_g) != which(grid_lrns$str_m == str_m)) {
# SVM need an explicit mention what kind of classif/regr will be performed
if(str_detect(str_g, "\\.svm$")) {
lrn_g = exec(lrn, !!!str_g, type = if_else(str_type_y == "regr", "eps-regression", "C-classification"))
} else {
lrn_g = exec(lrn, !!!str_g)
}
if(str_detect(str_m, "\\.svm$")) {
lrn_m = exec(lrn, !!!str_m, type = if_else(str_type_d == "regr", "eps-regression", "C-classification"))
} else {
lrn_m = exec(lrn, !!!str_m)
}
dml_est <- exec(
dml_class$new,
data = data_dml,
ml_g = lrn_g,
ml_m = lrn_m,
!!!list_dml_args,
)
if (!list_dml_args$draw_sample_splitting) {
dml_est$set_sample_splitting(list_samples)
}
if (rsmp_key != "no_cf") {
# Get optimal parameters
list_params_ml_g <- list_tuning %>%
pluck(str_remove(str_g, "^.*\\.")) %>%
pluck("Settings") %>%
pluck("Tuning Result") %>%
pluck(d_cols) %>%
map(~ .x$ml_g$params)
list_params_ml_m <- list_tuning %>%
pluck(str_remove(str_m, "^.*\\.")) %>%
pluck("Settings") %>%
pluck("Tuning Result") %>%
pluck(d_cols) %>%
map(~ .x$ml_m$params)
lgl_set_fold_specific <- TRUE
} else {
list_params_ml_g <- list_tuning[[str_remove(str_g, "^.*\\.")]] %>%
pluck("Tuning_Results") %>%
filter(fun == "ml_g") %>%
pluck("learner_param_vals") %>%
pluck(1)
list_params_ml_m <- list_tuning[[str_remove(str_m, "^.*\\.")]] %>%
pluck("Tuning_Results") %>%
filter(fun == "ml_m") %>%
pluck("learner_param_vals") %>%
pluck(1)
lgl_set_fold_specific <- FALSE
}
dml_est$set_ml_nuisance_params("ml_g", d_cols, list_params_ml_g,
set_fold_specific = lgl_set_fold_specific)
dml_est$set_ml_nuisance_params("ml_m", d_cols, list_params_ml_m,
set_fold_specific = lgl_set_fold_specific)
ddpcr::quiet(dml_est$fit(store_predictions = TRUE))
df_estimates_best <- data.frame(
parameter_est = dml_est$coef,
sd = dml_est$se,
df = NA,
p_value = dml_est$pval
)
if (int_repeats > 1) {
vec_mean <- dml_mean(dml_est, na.rm)
df_estimates_best <- df_estimates_best %>%
rbind(vec_mean)
}
df_estimates$ml_g <- dml_est$learner$ml_g$id
df_estimates$ml_m <- dml_est$learner$ml_m$id
df_estimates$time_tuning <- NA
list_settings <- list(
`DML algorithm` = dml_est$dml_procedure,
`N Folds` = dml_est$n_folds,
`N Rep` = dml_est$n_rep,
`Learner` = dml_est$learner
)
# Now append the results from the best model
df_estimates <- df_estimates %>%
rbind(df_estimates_best)
list_settings_all <- list_settings_all %>%
append(
list(
list_settings
)
)
names(list_settings_all)[length(list_settings_all)] <- str_name_best
} else {
# Just Rename the elements that are best
str_best_pattern <- paste0("g: ", str_g, " m: ", str_m, "$")
names(list_settings_all)[str_detect(names(list_settings_all), str_best_pattern)] <- str_name_best
}
# Mark for estimates wihich has lowest estimated prediction error
df_estimates <- df_estimates %>%
mutate(
algorithms = case_when(
ml_g == str_g & ml_m == str_m ~ "Best",
TRUE ~ paste0("G: ", str_remove(ml_g, "^.*\\."), " M: ", str_remove(ml_m, "^.*\\."))
)
)
# Return a list
list(
Estimates = df_estimates,
Settings = list_settings_all,
Measures = df_msrs
)
}
dml_no_cf_no_tune <- dml_estimator(
dataset = df_neural, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = FALSE,
tune_settings = list_tune_settings_cv,
par_grids = list_parameterspace,
rsmp_key = "no_cf",
list_globals = list_globals
)
dml_no_cf_no_tune
dml_no_cf_no_tune$Estimates
dml_no_cf_tune <- dml_estimator(
dataset = df_neural, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
tune_settings = list_tune_settings_cv,
par_grids = list_parameterspace,
rsmp_key = "no_cf",
list_globals = list_globals
)
dml_no_cf_tune$Estimates
dml_no_cf_tune$Measures
dml_no_cf_tune_non_orth <- dml_estimator(
dataset = df_neural, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
tune_settings = list_tune_settings_cv,
par_grids = list_parameterspace,
rsmp_key = "no_cf",
list_globals = list_globals,
score = non_orth_score
)
dml_no_cf_tune_non_orth$Estimates
dml_no_cf_tune_non_orth$Measures
# Normal splitting
dml_cf_cv_tune <- dml_estimator(
dataset = df_neural, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
tune_settings = list_tune_settings_cv,
par_grids = list_parameterspace,
rsmp_key = "cv", rsmp_args = list(nfolds = 5),
list_globals = list_globals
)
# Normal splitting
dml_cf_cv_tune <- dml_estimator(
dataset = df_neural, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
tune_settings = list_tune_settings_cv,
par_grids = list_parameterspace,
rsmp_key = "cv", rsmp_args = list(folds = 5),
list_globals = list_globals
)
quick_save <- function(x, folder = "Data/"){
str_x_name <- deparse(quote(x))
str_path <- paste0(folder, str_x_name, ".RData")
save(x, file = str_path)
}
x <- rnorm(10)
quick_save(x)
dml_cf_cv_tune$Estimates
dml_cf_cv_tune$Measures
# Repeated Cross Fitting
dml_cf_rcv_tune <- dml_estimator(
dataset = df_neural, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
tune_settings = list_tune_settings_cv,
par_grids = list_parameterspace,
rsmp_key = "repeated_cv", rsmp_args = list(folds = 5, rep = 5),
list_globals = list_globals
)
# Repeated Cross Fitting
dml_cf_rcv_tune <- dml_estimator(
dataset = df_neural, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
tune_settings = list_tune_settings_cv,
par_grids = list_parameterspace,
rsmp_key = "repeated_cv", rsmp_args = list(folds = 5, repeats = 5),
list_globals = list_globals
)
dml_cf_rcv_tune$Estimates
dml_cf_rcv_tune$Measures
rm(list = ls())
library(DoubleML)
library(mlr3verse)
library(mlr3extralearners)
library(tidyverse)
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Detect number of cores
int_cores <- parallel::detectCores() - 1
# Specify setup
vec_ml_g <- c("regr.xgboost")
vec_ml_m <- c("classif.xgboost")
vec_X_cols <- paste0("X.", 1:30)
vec_D_col <- "D"
vec_Y_col <- "Y"
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 10),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
source("Code/Definition Parameter Space.R")
rm(list = ls())
source("Code/Definition Parameter Space.R")
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Detect number of cores
int_cores <- parallel::detectCores() - 1
# Specify setup
vec_ml_g <- c("regr.xgboost")
vec_ml_m <- c("classif.xgboost")
vec_X_cols <- paste0("X.", 1:30)
vec_D_col <- "D"
vec_Y_col <- "Y"
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 10),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
# No sample splitting
load("Data/Sparse.RData")
sparse <- sparse %>% subset(N = c(50, 100, 400), Samples = 1:3)
mcs_sparse <- mcs(dml_estimator, sparse)
# Remove sparse to keep working memory ready
rm(sparse)
mcs_sparse_naive <- mcs_sparse %>%
run_simulation(
seed = 10,
parallel = TRUE,
workers = int_cores,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "cv",
rsmp_args = list(folds = 5),
par_grids = list_parameterspace, tune_settings = list_tune_settings,
list_globals = list_globals,
score = non_orth_score,
apply_cross_fitting = FALSE
)
run_simulation.mcs
mcs_sparse_naive <- mcs_sparse %>%
run_simulation(
seed = 10,
workers = int_cores,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "cv",
rsmp_args = list(folds = 5),
par_grids = list_parameterspace, tune_settings = list_tune_settings,
list_globals = list_globals,
score = non_orth_score,
apply_cross_fitting = FALSE
)
library(DoubleML)
library(mlr3verse)
library(mlr3extralearners)
library(tidyverse)
source("Code/Definition Parameter Space.R")
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Detect number of cores
int_cores <- parallel::detectCores() - 1
# Specify setup
vec_ml_g <- c("regr.xgboost")
vec_ml_m <- c("classif.xgboost")
vec_X_cols <- paste0("X.", 1:30)
vec_D_col <- "D"
vec_Y_col <- "Y"
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 10),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
# No sample splitting
load("Data/Sparse.RData")
sparse <- sparse %>% subset(N = c(50, 100, 400), Samples = 1:3)
mcs_sparse <- mcs(dml_estimator, sparse)
# Remove sparse to keep working memory ready
rm(sparse)
mcs_sparse_naive <- mcs_sparse %>%
run_simulation(
seed = 10,
workers = int_cores,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "cv",
rsmp_args = list(folds = 5),
par_grids = list_parameterspace, tune_settings = list_tune_settings,
list_globals = list_globals,
score = non_orth_score,
apply_cross_fitting = FALSE
)
mcs_sparse_naive$dgp$datasets <- NULL
quick_save(mcs_sparse_naive)
rm(mcs_sparse_naive)
mcs_sparse_non_orth <- mcs_sparse %>%
run_simulation(
seed = 10,
parallel = TRUE,
workers = int_cores,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "cv",
rsmp_args = list(folds = 5),
par_grids = list_parameterspace, tune_settings = list_tune_settings,
list_globals = list_globals,
score = non_orth_score,
)
mcs_sparse_non_orth$dgp$datasets <- NULL
quick_save(mcs_sparse_non_orth)
rm(mcs_sparse_non_orth)
mcs_sparse_non_cf <- mcs_sparse %>%
run_simulation(
seed = 10,
parallel = TRUE,
workers = int_cores,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "cv",
rsmp_args = list(folds = 5),
par_grids = list_parameterspace, tune_settings = list_tune_settings,
list_globals = list_globals,
apply_cross_fitting = FALSE
)
mcs_sparse_non_cf$dgp$datasets <- NULL
quick_save(mcs_sparse_non_cf)
rm(mcs_sparse_non_cf)
mcs_sparse_dml <- mcs_sparse %>%
run_simulation(
seed = 10,
parallel = TRUE,
workers = int_cores,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "cv",
rsmp_args = list(folds = 5),
par_grids = list_parameterspace, tune_settings = list_tune_settings,
list_globals = list_globals,
)
rm(list = ls())
# Add no tuning case
library(DoubleML)
library(mlr3verse)
library(mlr3extralearners)
library(tidyverse)
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Detect number of cores
int_cores <- parallel::detectCores() - 1
# Specify setup
vec_ml_g <- c("regr.xgboost")
vec_ml_m <- c("classif.xgboost")
vec_X_cols <- paste0("X.", 1:30)
vec_D_col <- "D"
vec_Y_col <- "Y"
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 50),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
source("Code/Definition Parameter Space.R")
library(DoubleML)
library(mlr3verse)
library(mlr3extralearners)
library(tidyverse)
rm(list = ls())
source("Code/Definition Parameter Space.R")
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Detect number of cores
int_cores <- parallel::detectCores() - 1
# Specify setup
vec_ml_g <- c("regr.xgboost")
vec_ml_m <- c("classif.xgboost")
vec_X_cols <- paste0("X.", 1:30)
vec_D_col <- "D"
vec_Y_col <- "Y"
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 10),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
# No sample splitting
load("Data/Sparse.RData")
.libPaths()
