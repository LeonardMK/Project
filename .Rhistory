group_by(fun, mle) %>%
summarise(
mean_msr_in = mean(msr, na.rm = na.rm)
) %>%
left_join(df_tune_result_out, by = c("fun", "mle")) %>%
mutate(
min_msr_in = case_when(
fun == "ml_g" & mean_msr_in == min(mean_msr_in) ~ TRUE,
fun == "ml_m" & mean_msr_in == min(mean_msr_in) ~ TRUE,
TRUE ~ FALSE
),
min_msr_val = case_when(
fun == "ml_g" & mean_msr_val == min(mean_msr_val) ~ TRUE,
fun == "ml_m" & mean_msr_val == min(mean_msr_val) ~ TRUE,
TRUE ~ FALSE
)
# One dataframe containing results
df_time <- map(list_tuning, ~ c(time_tuning = .x$Time)) %>%
map_df(~ .x)
list_estimates <- list_tuning %>%
map(~{
.x$Estimates
})
df_estimates <- do.call(rbind.data.frame, list_estimates) %>%
set_rownames(NULL) %>%
mutate(
ml_g = rep(ml_g, each = 2),
ml_m = rep(ml_m, each = 2),
) %>%
cbind(time_tuning = rep(df_time$time_tuning, each = 2))
# A list containing lists with specifications and predictions
list_settings_all <- list_tuning %>%
map(~ .x %>% pluck("Settings")) %>%
set_names(
c(
paste0("g: ", ml_g, " m: ", ml_m)
)
# List containing predictions
list_predictions_all <- list_tuning %>%
map(~ .x %>% pluck("Predictions")) %>%
set_names(
c(
paste0("g: ", ml_g, " m: ", ml_m)
)
# Find the best ml estimators
str_g <- df_msrs %>% filter(fun == "ml_g", min_msr_val) %>% pull(mle)
str_m <- df_msrs %>% filter(fun == "ml_m", min_msr_val) %>% pull(mle)
str_name_best <- paste0("g best: ", str_g, " m best: ", str_m, collapse = "")
if (which(grid_lrns$str_g == str_g) != which(grid_lrns$str_m == str_m)) {
# Create a new dml object with said algorithms and the given specifications
# SVM need an explicit mention what kind of classif/regr will be performed
if(str_detect(str_g, "\\.svm$")) {
lrn_g = exec(lrn, !!!str_g, type = if_else(str_type_y == "regr", "eps-regression", "C-classification"))
} else {
lrn_g = exec(lrn, !!!str_g)
}
if(str_detect(str_m, "\\.svm$")) {
lrn_m = exec(lrn, !!!str_m, type = if_else(str_type_d == "regr", "eps-regression", "C-classification"))
} else {
lrn_m = exec(lrn, !!!str_m)
}
dml_est <- dml_class$new(
data_dml,
lrn_g,
lrn_m,
...,
draw_sample_splitting = draw_sample_splitting
)
if (!draw_sample_splitting) dml_est$set_sample_splitting(list_samples)
# Get optimal parameters
list_params_ml_g <- list_tuning %>%
pluck(str_remove(str_g, "^.*\\.")) %>%
pluck("Settings") %>%
pluck("Tuning Result") %>%
pluck(d_cols) %>%
map(~ .x$ml_g$params)
list_params_ml_m <- list_tuning %>%
pluck(str_remove(str_m, "^.*\\.")) %>%
pluck("Settings") %>%
pluck("Tuning Result") %>%
pluck(d_cols) %>%
map(~ .x$ml_m$params)
dml_est$set_ml_nuisance_params("ml_g", d_cols, list_params_ml_g, set_fold_specific = TRUE)
dml_est$set_ml_nuisance_params("ml_m", d_cols, list_params_ml_m, set_fold_specific = TRUE)
ddpcr::quiet(dml_est$fit(store_predictions = TRUE))
vec_mean <- dml_mean(dml_est, na.rm)
df_estimates_best <- cbind.data.frame(
parameter_est = c(dml_est$coef, vec_mean["parameter_est"]),
sd = c(dml_est$se, vec_mean["sd"]),
df = NA,
p_value = c(dml_est$pval, vec_mean["p_value"]),
ml_g = dml_est$learner$ml_g$id,
ml_m = dml_est$learner$ml_m$id,
time_tuning = NA
)
list_settings <- list(
`DML algorithm` = dml_est$dml_procedure,
`N Folds` = dml_est$n_folds,
`N Rep` = dml_est$n_rep,
`Learner` = dml_est$learner,
`Tuning Result` = dml_est$tuning_res,
`Sample Splits` = dml_est$smpls
)
list_predictions <- dml_est$predictions
names(list_predictions) <- str_remove(names(list_predictions), "ml_")
# Now append the results from the best model
df_estimates <- df_estimates %>%
rbind(df_estimates_best)
list_settings_all <- list_settings_all %>%
append(
list(
list_settings
)
list_predictions_all <- list_predictions_all %>%
append(
list(
list_predictions
)
names(list_settings_all)[length(list_settings_all)] <- str_name_best
names(list_predictions_all)[length(list_predictions_all)] <- str_name_best
} else {
# Just Rename the elements that are best
str_best_pattern <- paste0("g: ", str_g, " m: ", str_m, "$")
names(list_settings_all)[str_detect(names(list_settings_all), str_best_pattern)] <- str_name_best
names(list_predictions_all)[str_detect(names(list_predictions_all), str_best_pattern)] <- str_name_best
}
# Mark for estimates wihich has lowest estimated prediction error
df_estimates <- df_estimates %>%
mutate(
algorithms = case_when(
ml_g == str_g & ml_m == str_m ~ "Best",
TRUE ~ paste0("G: ", str_remove(ml_g, "^.*\\."), " M: ", str_remove(ml_m, "^.*\\."))
)
# Return a list
list(
Estimates = df_estimates,
Settings = list_settings_all,
Predictions = list_predictions_all,
Measures = df_msrs
)
}
mcs_small_rob <- mcs_small %>%
run_simulation(
seed = 2, parallel = FALSE,
x_cols = c("X.1", "X.2"), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = FALSE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger)
)
mcs_small <- mcs(dml_estimator, dgp_small)
mcs_small_rob <- mcs_small %>%
run_simulation(
seed = 2, parallel = TRUE, workers = 3
x_cols = c("X.1", "X.2"), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = FALSE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger)
)
mcs_small_rob <- mcs_small %>%
run_simulation(
seed = 2, parallel = TRUE, workers = 3,
x_cols = c("X.1", "X.2"), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = FALSE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger)
)
mcs_small_rob <- mcs_small %>%
run_simulation(
seed = 2, parallel = FALSE,
x_cols = c("X.1", "X.2"), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = FALSE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger)
)
source("Code/Monte Carlo class.R")
source("Code/Estimator Functions.R")
mcs_small <- mcs(dml_estimator, dgp_small)
mcs_small_rob <- mcs_small %>%
run_simulation(
seed = 2, parallel = FALSE,
x_cols = c("X.1", "X.2"), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = FALSE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger)
)
mcs_small_rob <- mcs_small %>%
run_simulation(
seed = 2, parallel = TRUE, workers = 3,
x_cols = c("X.1", "X.2"), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = FALSE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger)
)
mcs_small_rob <- mcs_small %>%
run_simulation(
seed = 2, parallel = TRUE, workers = 3,
x_cols = c("X.1", "X.2"), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = TRUE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger)
)
globalenv()
# Run MCS function. Allows for parallel execution
run_simulation.mcs <- function(mcs_obj, seed, samples = NULL, N = NULL,
parallel = FALSE, workers, .env_globals = parent.frame(), ...){
# Create datasets if not already present
if (length(mcs_obj$dgp$datasets) == 0) {
mcs_obj$dgp <- mcs_obj$dgp %>% run_simulation(seed, samples, N)
}
if (parallel) {
plan(multisession, workers = workers)
list_estimates <- future_map(mcs_obj$dgp$datasets, function(dataset){
list(
Output = mcs_obj$estimator(dataset$data, ...),
N = dataset$N,
Sample = dataset$Sample
)
},
.options = furrr_options(
packages = sessionInfo() %>% pluck("otherPkgs") %>% names(),
globals = globals,
seed = seed
),
.progress = TRUE)
} else {
set.seed(seed)
list_estimates <- map(mcs_obj$dgp$datasets, function(dataset){
list(
Output = mcs_obj$estimator(dataset$data, ...),
N = dataset$N,
Sample = dataset$Sample)
})
}
# Keep estimates in whatever form. This allows for more flexibility
names(list_estimates) <- mcs_obj$dgp$datasets %>% names()
mcs_obj$results <- list_estimates
mcs_obj
}
mcs_small <- mcs(dml_estimator, dgp_small)
mcs_small_rob <- mcs_small %>%
run_simulation(
seed = 2, parallel = TRUE, workers = 3,
x_cols = c("X.1", "X.2"), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = TRUE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger)
)
# Run MCS function. Allows for parallel execution
run_simulation.mcs <- function(mcs_obj, seed, samples = NULL, N = NULL,
parallel = FALSE, workers, .env_globals = parent.frame(), ...){
# Create datasets if not already present
if (length(mcs_obj$dgp$datasets) == 0) {
mcs_obj$dgp <- mcs_obj$dgp %>% run_simulation(seed, samples, N)
}
if (parallel) {
plan(multisession, workers = workers)
list_estimates <- future_map(mcs_obj$dgp$datasets, function(dataset){
list(
Output = mcs_obj$estimator(dataset$data, ...),
N = dataset$N,
Sample = dataset$Sample
)
},
.options = furrr_options(
packages = sessionInfo() %>% pluck("otherPkgs") %>% names(),
.env_globals = .env_globals,
seed = seed
),
.progress = TRUE)
} else {
set.seed(seed)
list_estimates <- map(mcs_obj$dgp$datasets, function(dataset){
list(
Output = mcs_obj$estimator(dataset$data, ...),
N = dataset$N,
Sample = dataset$Sample)
})
}
# Keep estimates in whatever form. This allows for more flexibility
names(list_estimates) <- mcs_obj$dgp$datasets %>% names()
mcs_obj$results <- list_estimates
mcs_obj
}
mcs_small <- mcs(dml_estimator, dgp_small)
mcs_small_rob <- mcs_small %>%
run_simulation(
seed = 2, parallel = TRUE, workers = 3,
x_cols = c("X.1", "X.2"), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = TRUE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger)
)
# Run MCS function. Allows for parallel execution
run_simulation.mcs <- function(mcs_obj, seed, samples = NULL, N = NULL,
parallel = FALSE, workers, .env_globals = parent.frame(), ...){
# Create datasets if not already present
if (length(mcs_obj$dgp$datasets) == 0) {
mcs_obj$dgp <- mcs_obj$dgp %>% run_simulation(seed, samples, N)
}
if (parallel) {
plan(multisession, workers = workers)
list_estimates <- future_map(mcs_obj$dgp$datasets, function(dataset){
list(
Output = mcs_obj$estimator(dataset$data, ...),
N = dataset$N,
Sample = dataset$Sample
)
},
.options = furrr_options(
packages = sessionInfo() %>% pluck("otherPkgs") %>% names(),
seed = seed
),
.env_globals = .env_globals
.progress = TRUE)
} else {
set.seed(seed)
list_estimates <- map(mcs_obj$dgp$datasets, function(dataset){
list(
Output = mcs_obj$estimator(dataset$data, ...),
N = dataset$N,
Sample = dataset$Sample)
})
}
# Keep estimates in whatever form. This allows for more flexibility
names(list_estimates) <- mcs_obj$dgp$datasets %>% names()
mcs_obj$results <- list_estimates
mcs_obj
}
# Run MCS function. Allows for parallel execution
run_simulation.mcs <- function(mcs_obj, seed, samples = NULL, N = NULL,
parallel = FALSE, workers, .env_globals = parent.frame(), ...){
# Create datasets if not already present
if (length(mcs_obj$dgp$datasets) == 0) {
mcs_obj$dgp <- mcs_obj$dgp %>% run_simulation(seed, samples, N)
}
if (parallel) {
plan(multisession, workers = workers)
list_estimates <- future_map(mcs_obj$dgp$datasets, function(dataset){
list(
Output = mcs_obj$estimator(dataset$data, ...),
N = dataset$N,
Sample = dataset$Sample
)
},
.options = furrr_options(
packages = sessionInfo() %>% pluck("otherPkgs") %>% names(),
seed = seed
),
.env_globals = .env_globals,
.progress = TRUE)
} else {
set.seed(seed)
list_estimates <- map(mcs_obj$dgp$datasets, function(dataset){
list(
Output = mcs_obj$estimator(dataset$data, ...),
N = dataset$N,
Sample = dataset$Sample)
})
}
# Keep estimates in whatever form. This allows for more flexibility
names(list_estimates) <- mcs_obj$dgp$datasets %>% names()
mcs_obj$results <- list_estimates
mcs_obj
}
mcs_small_rob <- mcs_small %>%
run_simulation(
seed = 2, parallel = TRUE, workers = 3,
x_cols = c("X.1", "X.2"), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = TRUE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger)
)
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Run MCS function. Allows for parallel execution
run_simulation.mcs <- function(mcs_obj, seed, samples = NULL, N = NULL,
parallel = FALSE, workers, .env_globals = parent.frame(), ...){
# Create datasets if not already present
if (length(mcs_obj$dgp$datasets) == 0) {
mcs_obj$dgp <- mcs_obj$dgp %>% run_simulation(seed, samples, N)
}
if (parallel) {
plan(multisession, workers = workers)
list_estimates <- future_map(mcs_obj$dgp$datasets, function(dataset){
list(
Output = mcs_obj$estimator(dataset$data, ...),
N = dataset$N,
Sample = dataset$Sample
)
},
.options = furrr_options(
packages = sessionInfo() %>% pluck("otherPkgs") %>% names(),
seed = seed
),
.env_globals = .env_globals,
.progress = TRUE)
} else {
set.seed(seed)
list_estimates <- map(mcs_obj$dgp$datasets, function(dataset){
list(
Output = mcs_obj$estimator(dataset$data, ...),
N = dataset$N,
Sample = dataset$Sample)
})
}
# Keep estimates in whatever form. This allows for more flexibility
names(list_estimates) <- mcs_obj$dgp$datasets %>% names()
mcs_obj$results <- list_estimates
mcs_obj
}
mcs_small <- mcs(dml_estimator, dgp_small)
library(DoubleML)
library(MASS)
library(mlr3verse)
library(plotly)
library(DoubleML)
library(MASS)
library(mlr3verse)
library(plotly)
library(tidyverse)
setwd("C:/Users/Wilms/OneDrive - uni-bonn.de/Uni Bonn/6. Semester/Masterarbeit/Project/")
library(DoubleML)
library(mlr3verse)
library(tidyverse)
setwd("C:/Users/Wilms/OneDrive - uni-bonn.de/Uni Bonn/6. Semester/Masterarbeit/Project/")
source("Code/DGP class.R")
source("Code/DGP functions.R")
install.packages(c("sigmoid", "formula.tools"))
source("Code/DGP class.R")
install.packages("ggpubr")
source("Code/DGP class.R")
source("Code/DGP functions.R")
source("Code/Definition Parameter Space.R")
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
install.packages("furrr")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Simple example to compare scores
set.seed(3141)
int_N <- 10000
int_p <- 4
theta <- 0.6
vec_mean <- runif(int_p, -1, 1)
mat_A <- matrix(runif(int_p ^ 2, -1, 1), nrow = int_p, ncol = int_p)
mat_Sigma <- cov2cor(t(mat_A) %*% mat_A)
X <- MASS::mvrnorm(int_N, vec_mean, mat_Sigma)
colnames(X) <- paste0("X", 1:int_p)
U <- rnorm(int_N)
V <- rnorm(int_N)
prob_D <- dgp_nnet(X, V, beta_0 = -2, type = "classification")
# Simple example to compare scores
load("Data/Sine.RData")
# Keep only N = 100
sine$datasets[1001]
# Keep only N = 100
sine$datasets <- sine$datasets[1001:2000]
list_tune_settings <- list(
terminator = trm("evals", n_evals = 10),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
dataset <- sine$datasets[[1]]$data
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger"
)
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger",
list_globals = NULL
)
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger",
list_globals = list(NULL)
)
install.packages("ddpcr")
