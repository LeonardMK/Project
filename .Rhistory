sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_sparse) / var_U
fac_m_sparse <- 2.7
rbinom(int_N, 1, sigmoid(m_sparse + vec_U)) %>% table()
# Sine
m_sine <- dgp_sine(df_X, vec_U, vec_gamma, "classification", 2.5) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_sine) / var_U
# Sine
m_sine <- dgp_sine(df_X, vec_U, vec_gamma, "classification", 3) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_sine) / var_U
# Sine
m_sine <- dgp_sine(df_X, vec_U, vec_gamma, "classification", 2.9) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_sine) / var_U
# Sine
m_sine <- dgp_sine(df_X, vec_U, vec_gamma, "classification", 2.8) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_sine) / var_U
# Sine
m_sine <- dgp_sine(df_X, vec_U, vec_gamma, "classification", 2.9) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_sine) / var_U
fac_m_sine <- 2.9
rbinom(int_N, 1, sigmoid(m_sine + vec_U)) %>% table()
# Polynomial
int_order <- 2
m_poly <- dgp_poly(df_X, vec_U, order = int_order,
type = "classification", factor_signal = 0.01) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_poly) / var_U
m_poly <- dgp_poly(df_X, vec_U, order = int_order,
type = "classification", factor_signal = 0.1) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_poly) / var_U
m_poly <- dgp_poly(df_X, vec_U, order = int_order,
type = "classification", factor_signal = 0.05) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_poly) / var_U
m_poly <- dgp_poly(df_X, vec_U, order = int_order,
type = "classification", factor_signal = 0.04) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_poly) / var_U
m_poly <- dgp_poly(df_X, vec_U, order = int_order,
type = "classification", factor_signal = 0.049) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_poly) / var_U
m_poly <- dgp_poly(df_X, vec_U, order = int_order,
type = "classification", factor_signal = 0.048) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_poly) / var_U
fac_m_poly <- 0.048
rbinom(int_N, 1, sigmoid(m_poly + vec_U)) %>% table()
rbinom(int_N, 1, sigmoid(m_poly + vec_U - 0.1)) %>% table()
rbinom(int_N, 1, sigmoid(m_poly + vec_U - 0.2)) %>% table()
rbinom(int_N, 1, sigmoid(m_poly + vec_U - 0.5)) %>% table()
summary(m_poly)
rbinom(int_N, 1, sigmoid(m_poly + vec_U - 3)) %>% table()
rbinom(int_N, 1, sigmoid(m_poly + vec_U - 2)) %>% table()
rbinom(int_N, 1, sigmoid(m_poly + vec_U - 1)) %>% table()
rbinom(int_N, 1, sigmoid(m_poly + vec_U - 1.2)) %>% table()
rbinom(int_N, 1, sigmoid(m_poly + vec_U - 1.1)) %>% table()
scale_m_poly <- -1.1
# Neural Net
int_hidden_units <- 5
dbl_beta_0 <- -3
m_nnet <- dgp_nnet(df_X, vec_U, type = "classification",
beta_0 = dbl_beta_0, factor_signal = 1) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_nnet) / var_U
m_nnet <- dgp_nnet(df_X, vec_U, type = "classification",
beta_0 = dbl_beta_0, factor_signal = 0.95) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_nnet) / var_U
m_nnet <- dgp_nnet(df_X, vec_U, type = "classification",
beta_0 = dbl_beta_0, factor_signal = 0.96) %>%
sigmoid(inverse = TRUE) %>%
subtract(vec_U)
var(m_nnet) / var_U
fac_m_nnet <- 0.96
rbinom(int_N, 1, sigmoid(m_nnet + vec_U)) %>% table()
# Define sample sizes
vec_N <- c(50, 100, 400, 800, 1600)
# Sparse Matrix
sparse <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E, beta),
theta = 0.6,
g = function(X, E, beta) dgp_sparse(X, E, beta, "regression", fac_g_sparse),
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
beta = vec_beta_sparse,
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U, gamma),
m = function(X, U, gamma){
Prob_D <- dgp_sparse(fac_m_sparse * X, U, gamma, "classification", fac_m_sparse)
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
gamma = vec_gamma_sparse
)
sparse <- sparse %>%
run_simulation(N = 10000, seed = 2, samples = 1)
# Check whether properties hold
data_sparse <- sparse$datasets$`Sample = 1 with N = 10000`$data
var(data_sparse$Y - data_sparse$E) / var(data_sparse$E)
var(data_sparse$Prob_D %>% sigmoid(inverse = TRUE) - data_sparse$U) / var(data_sparse$U)
fac_m_sparse
# Sparse Matrix
sparse <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E, beta),
theta = 0.6,
g = function(X, E, beta) dgp_sparse(X, E, beta, "regression", fac_g_sparse),
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
beta = vec_beta_sparse,
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U, gamma),
m = function(X, U, gamma){
Prob_D <- dgp_sparse(X, U, gamma, "classification", fac_m_sparse)
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
gamma = vec_gamma_sparse
)
sparse <- sparse %>%
run_simulation(N = 10000, seed = 2, samples = 1)
# Check whether properties hold
data_sparse <- sparse$datasets$`Sample = 1 with N = 10000`$data
var(data_sparse$Y - data_sparse$E) / var(data_sparse$E)
var(data_sparse$Prob_D %>% sigmoid(inverse = TRUE) - data_sparse$U) / var(data_sparse$U)
data_sparse$D %>% table()
# Create whole dgp set
sparse <- sparse %>%
run_simulation(N = vec_N, seed = 2, samples = int_samples)
save(sparse, file = "Data/Sparse.RData")
rm(sparse)
# Sine Function
sine <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E, beta),
theta = 0.6,
g = function(X, E, beta) dgp_sine(X, E, beta, "regression", fac_g_sine),
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
beta = vec_beta,
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U, gamma),
m = function(X, U, gamma){
Prob_D <- dgp_sine(X, U, gamma, "classification", fac_m_sine)
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
gamma = vec_gamma
)
# Check whether properties hold
sine <- sine %>%
run_simulation(N = 10000, seed = 2, samples = 1)
data_sine <- sine$datasets$`Sample = 1 with N = 10000`$data
var(data_sine$Y - data_sine$E) / var(data_sine$E)
var(data_sine$Prob_D %>% sigmoid(inverse = TRUE) - data_sine$U) / var(data_sine$U)
data_sine$D %>% table()
# Create whole dgp set
sine <- sine %>%
run_simulation(N = vec_N, seed = 2, samples = int_samples)
save(sine, file = "Data/Sine.RData")
rm(sine)
# Interaction Function
inter <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E, beta),
beta_0 = 1.5,
theta = 0.6,
g = function(X, E) {
dgp_poly(X, E, order = 3, type = "regression", factor_signal = fac_g_poly)
},
X = MASS::mvrnorm(N, mu = vec_mu + dbl_mu_poly, Sigma = mat_Sigma),
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U),
m = function(X, U){
Prob_D <- dgp_poly(X, U, order = 3, type = "classification",
factor_signal = fac_m_poly)
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
)
sd_U
# Check whether properties hold
poly <- poly %>%
run_simulation(N = 10000, seed = 2, samples = 1)
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 10000, seed = 2, samples = 1)
fac_g_poly
fac_m_poly
# Interaction Function
inter <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E, beta),
beta_0 = 1.5,
theta = 0.6,
g = function(X, E) {
dgp_poly(X, E, order = 3, type = "regression", factor_signal = fac_g_poly)
},
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U),
m = function(X, U){
Prob_D <- dgp_poly(X, U, order = 3, type = "classification",
factor_signal = fac_m_poly)
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
)
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 10000, seed = 2, samples = 1)
data_inter <- inter$datasets$`Sample = 1 with N = 10000`$data
var(data_inter$Y - data_inter$E) / var(data_inter$E)
data_inter <- inter$datasets$`Sample = 1 with N = 10000`$data
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 10000, seed = 2, samples = 1)
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 1000, seed = 2, samples = 1)
# Interaction Function
inter <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E, beta),
beta_0 = 1.5,
theta = 0.6,
g = function(X, E) {
dgp_poly(X, E, order = 2, type = "regression", factor_signal = fac_g_poly)
},
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U),
m = function(X, U){
Prob_D <- dgp_poly(X, U, order = 2, type = "classification",
factor_signal = fac_m_poly)
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
)
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 10000, seed = 2, samples = 1)
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 10000, seed = 2, samples = 1)
str_formula
ncol(X)
head(X)
as.data.frame(X) %>% colnames()
class(X)
data.frame(X)
mat_X <- model.frame(as.formula(paste0("~ ", str_formula)), data = data.frame(X))
# Function with high order moments using kernels
dgp_poly <- function(X, E = rnorm(nrow(X)),
order = 2,
beta = rep(1, (sum(ncol(X) ^ seq(1, order)))),
type = c("regression", "classification"), factor_signal = 1){
if (!is.matrix(X)) X <- as.matrix(X)
# Calculate products feature products
str_formula <- create_interactions(X, order = order)
mat_X <- model.frame(as.formula(paste0("~ ", str_formula)), data = data.frame(X))
Y <- factor_signal * (as.matrix(mat_X) %*% beta) + E
if (type == "regression") {
as.vector(Y)
} else {
sigmoid(as.vector(Y))
}
# Interaction Function
inter <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E, beta),
beta_0 = 1.5,
theta = 0.6,
g = function(X, E) {
dgp_poly(X, E, order = 2, type = "regression", factor_signal = fac_g_poly)
},
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U),
m = function(X, U){
Prob_D <- dgp_poly(X, U, order = 2, type = "classification",
factor_signal = fac_m_poly)
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
)
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 10000, seed = 2, samples = 1)
# Interaction Function
inter <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E),
beta_0 = 1.5,
theta = 0.6,
g = function(X, E) {
dgp_poly(X, E, order = 2, type = "regression", factor_signal = fac_g_poly)
},
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U),
m = function(X, U){
Prob_D <- dgp_poly(X, U, order = 2, type = "classification",
factor_signal = fac_m_poly)
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
)
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 10000, seed = 2, samples = 1)
data_inter <- inter$datasets$`Sample = 1 with N = 10000`$data
var(data_inter$Y - data_inter$E) / var(data_inter$E)
var(data_inter$Prob_D %>% sigmoid(inverse = TRUE) - data_inter$U) / var(data_inter$U)
data_inter$D %>% table()
scale_m_poly
# Interaction Function
inter <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E),
beta_0 = 1.5,
theta = 0.6,
g = function(X, E) {
dgp_poly(X, E, order = 2, type = "regression", factor_signal = fac_g_poly)
},
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U),
m = function(X, U){
# Need to sclae untransformed values s.t. classes are balanced
Prob_D <- dgp_poly(X, U, order = 2, type = "regression",
factor_signal = fac_m_poly) %>%
subtract(-1.1) %>%
sigmoid()
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
)
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 10000, seed = 2, samples = 1)
data_inter <- inter$datasets$`Sample = 1 with N = 10000`$data
var(data_inter$Y - data_inter$E) / var(data_inter$E)
var(data_inter$Prob_D %>% sigmoid(inverse = TRUE) - data_inter$U) / var(data_inter$U)
data_inter$D %>% table()
# Interaction Function
inter <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E),
beta_0 = 1.5,
theta = 0.6,
g = function(X, E) {
dgp_poly(X, E, order = 2, type = "regression", factor_signal = fac_g_poly)
},
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U),
m = function(X, U){
# Need to sclae untransformed values s.t. classes are balanced
Prob_D <- dgp_poly(X, U, order = 2, type = "regression",
factor_signal = fac_m_poly) %>%
subtract(1.1) %>%
sigmoid()
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
)
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 10000, seed = 2, samples = 1)
data_inter <- inter$datasets$`Sample = 1 with N = 10000`$data
var(data_inter$Y - data_inter$E) / var(data_inter$E)
var(data_inter$Prob_D %>% sigmoid(inverse = TRUE) - data_inter$U) / var(data_inter$U)
# Check whether properties hold
inter <- inter %>%
run_simulation(N = 10000, seed = 2, samples = 1)
data_inter <- inter$datasets$`Sample = 1 with N = 10000`$data
var(data_inter$Y - data_inter$E) / var(data_inter$E)
var(data_inter$Prob_D %>% sigmoid(inverse = TRUE) - data_inter$U) / var(data_inter$U)
data_inter$D %>% table()
# Create whole dgp set
inter <- inter %>%
run_simulation(N = vec_N, seed = 2, samples = int_samples)
save(inter, file = "Data/inter.RData")
rm(inter)
library(DoubleML)
library(MASS)
library(mlr3verse)
library(plotly)
library(tidyverse)
setwd("C:/Users/Wilms/OneDrive - uni-bonn.de/Uni Bonn/6. Semester/Masterarbeit/Project/")
library(DoubleML)
# Neural Network
neural <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E),
theta = 0.6,
g = function(X, E){
dgp_nnet(X, E, hidden_units = 5, type = "regression",
factor_signal = fac_g_nnet)},
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U),
m = function(X, U){
Prob_D <- dgp_nnet(
X, U, hidden_units = 5, type = "classification",
beta_0 = dbl_beta_0, factor_signal = fac_m_nnet)
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
)
# Check whether properties hold
nnet <- nnet %>%
run_simulation(N = 10000, seed = 2, samples = 1)
# Check whether properties hold
neural <- neural %>%
run_simulation(N = 10000, seed = 2, samples = 1)
data_neural <- neural$datasets$`Sample = 1 with N = 10000`$data
var(data_neural$Y - data_neural$E) / var(data_neural$E)
var(data_neural$Prob_D %>% sigmoid(inverse = TRUE) - data_neural$U) / var(data_neural$U)
data_neural$D %>% table()
# Create whole dgp set
neural <- neural %>%
run_simulation(N = vec_N, seed = 2, samples = int_samples)
save(neural, file = "Data/neural.RData")
# Create whole dgp set
inter <- inter %>%
run_simulation(N = vec_N, seed = 2, samples = 10)
# Interaction Function
inter <- dgp() %>%
add_level(
formulas = Y ~ theta * D + g(X, E),
beta_0 = 1.5,
theta = 0.6,
g = function(X, E) {
dgp_poly(X, E, order = 2, type = "regression", factor_signal = fac_g_poly)
},
X = MASS::mvrnorm(N, mu = vec_mu, Sigma = mat_Sigma),
E = rnorm(N, 0, sd_E)
) %>%
add_level(
formulas = D ~ m(X, U),
m = function(X, U){
# Need to sclae untransformed values s.t. classes are balanced
Prob_D <- dgp_poly(X, U, order = 2, type = "regression",
factor_signal = fac_m_poly) %>%
subtract(1.1) %>%
sigmoid()
D <- rbinom(nrow(X), 1, Prob_D)
cbind(D = D, Prob_D = as.vector(Prob_D))
},
U = rnorm(N, 0, sd_U),
)
# Create whole dgp set
inter <- inter %>%
run_simulation(N = vec_N, seed = 2, samples = 10)
# Create whole dgp set
inter <- inter %>%
run_simulation(N = vec_N, seed = 2, samples = int_samples)
save(inter, file = "Data/Inter.RData")
