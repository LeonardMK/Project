mle = c(str_g, str_m)
) %>%
cbind(rbind(acc_g_0, acc_m_0)) %>%
set_rownames(NULL)
list(
Estimates = df_results,
Settings = list_settings,
Tuning_Results = dt_tuning_results,
Accuracy_Validation = df_val_set,
Time = dbl_time_taken
)
})
vec_mle_names_g <- str_remove(ml_g, "^.*\\.")
vec_mle_names_m <- str_remove(ml_m, "^.*\\.")
vec_names_list_tuning <- if_else(
vec_mle_names_g == vec_mle_names_m,
vec_mle_names_g,
paste0("g: ", vec_mle_names_g, " m: ", vec_mle_names_m))
names(list_tuning) <- vec_names_list_tuning
# Find best learner
dt_tune_result_in <- list_tuning %>%
map_dfr(~ {
.x$Tuning_Results %>%
dplyr::select(
str_msr_g,
str_msr_m,
fun,
rep,
fold,
mle,
learner_param_vals
)
})
df_tune_result_out <- list_tuning %>%
map(~ .x$Accuracy_Validation) %>%
map_df(~ .x)
# Want to get validation set performance
df_msrs <- dt_tune_result_in %>%
mutate(
msr = case_when(
fun == "ml_g" ~ !!sym(str_msr_g),
fun == "ml_m" ~ !!sym(str_msr_m)
)
) %>%
group_by(fun, mle) %>%
summarise(
mean_msr_in = mean(msr, na.rm = na.rm)
) %>%
left_join(df_tune_result_out, by = c("fun", "mle")) %>%
mutate(
min_msr_in = case_when(
fun == "ml_g" & mean_msr_in == min(mean_msr_in) ~ TRUE,
fun == "ml_m" & mean_msr_in == min(mean_msr_in) ~ TRUE,
TRUE ~ FALSE
),
min_msr_val = case_when(
fun == "ml_g" & mean_msr_val == min(mean_msr_val) ~ TRUE,
fun == "ml_m" & mean_msr_val == min(mean_msr_val) ~ TRUE,
TRUE ~ FALSE
)
# One dataframe containing results
df_time <- map(list_tuning, ~ c(time_tuning = .x$Time)) %>%
map_df(~ .x)
list_estimates <- list_tuning %>%
map(~{
.x$Estimates
})
df_estimates <- do.call(rbind.data.frame, list_estimates) %>%
set_rownames(NULL) %>%
mutate(
ml_g = rep(ml_g, each = if_else(int_repeats == 1, 1, 2)),
ml_m = rep(ml_m, each = if_else(int_repeats == 1, 1, 2)),
) %>%
cbind(time_tuning = rep(df_time$time_tuning, each = 2))
# A list containing lists with specifications and predictions
list_settings_all <- list_tuning %>%
map(~ .x %>% pluck("Settings")) %>%
set_names(
c(
paste0("g: ", ml_g, " m: ", ml_m)
)
# Find the best ml estimators
str_g <- df_msrs %>% filter(fun == "ml_g", min_msr_val) %>% pull(mle)
str_m <- df_msrs %>% filter(fun == "ml_m", min_msr_val) %>% pull(mle)
str_name_best <- paste0("g best: ", str_g, " m best: ", str_m, collapse = "")
if (which(grid_lrns$str_g == str_g) != which(grid_lrns$str_m == str_m)) {
# Create a new dml object with said algorithms and the given specifications
# SVM need an explicit mention what kind of classif/regr will be performed
if(str_detect(str_g, "\\.svm$")) {
lrn_g = exec(lrn, !!!str_g, type = if_else(str_type_y == "regr", "eps-regression", "C-classification"))
} else {
lrn_g = exec(lrn, !!!str_g)
}
if(str_detect(str_m, "\\.svm$")) {
lrn_m = exec(lrn, !!!str_m, type = if_else(str_type_d == "regr", "eps-regression", "C-classification"))
} else {
lrn_m = exec(lrn, !!!str_m)
}
dml_est <- dml_class$new(
data_dml,
lrn_g,
lrn_m,
...,
draw_sample_splitting = draw_sample_splitting
)
if (!draw_sample_splitting) dml_est$set_sample_splitting(list_samples)
# Get optimal parameters
list_params_ml_g <- list_tuning %>%
pluck(str_remove(str_g, "^.*\\.")) %>%
pluck("Settings") %>%
pluck("Tuning Result") %>%
pluck(d_cols) %>%
map(~ .x$ml_g$params)
list_params_ml_m <- list_tuning %>%
pluck(str_remove(str_m, "^.*\\.")) %>%
pluck("Settings") %>%
pluck("Tuning Result") %>%
pluck(d_cols) %>%
map(~ .x$ml_m$params)
dml_est$set_ml_nuisance_params("ml_g", d_cols, list_params_ml_g, set_fold_specific = TRUE)
dml_est$set_ml_nuisance_params("ml_m", d_cols, list_params_ml_m, set_fold_specific = TRUE)
ddpcr::quiet(dml_est$fit(store_predictions = TRUE))
vec_mean <- dml_mean(dml_est, na.rm)
df_estimates_best <- cbind.data.frame(
parameter_est = c(dml_est$coef, vec_mean["parameter_est"]),
sd = c(dml_est$se, vec_mean["sd"]),
df = NA,
p_value = c(dml_est$pval, vec_mean["p_value"]),
ml_g = dml_est$learner$ml_g$id,
ml_m = dml_est$learner$ml_m$id,
time_tuning = NA
)
list_settings <- list(
`DML algorithm` = dml_est$dml_procedure,
`N Folds` = dml_est$n_folds,
`N Rep` = dml_est$n_rep,
`Learner` = dml_est$learner,
)
# Now append the results from the best model
df_estimates <- df_estimates %>%
rbind(df_estimates_best)
list_settings_all <- list_settings_all %>%
append(
list(
list_settings
)
names(list_settings_all)[length(list_settings_all)] <- str_name_best
} else {
# Just Rename the elements that are best
str_best_pattern <- paste0("g: ", str_g, " m: ", str_m, "$")
names(list_settings_all)[str_detect(names(list_settings_all), str_best_pattern)] <- str_name_best
}
# Mark for estimates wihich has lowest estimated prediction error
df_estimates <- df_estimates %>%
mutate(
algorithms = case_when(
ml_g == str_g & ml_m == str_m ~ "Best",
TRUE ~ paste0("G: ", str_remove(ml_g, "^.*\\."), " M: ", str_remove(ml_m, "^.*\\."))
)
# Return a list
list(
Estimates = df_estimates,
Settings = list_settings_all,
Measures = df_msrs
)
}
mcs_timing %>% mse()
mcs_timing <- mcs(dml_estimator, neural)
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 1,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = FALSE,
rsmp_key = "repeated_cv",
rsmp_args = list(folds = 5, repeats = 2),
par_grids = list_parameterspace,
list_globals = list_globals
))
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 1,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = FALSE,
rsmp_key = "repeated_cv",
rsmp_args = list(folds = 5, repeats = 3),
par_grids = list_parameterspace,
list_globals = list_globals
))
763 * 1000
763 * 1000 / 60
763 * 1000 / 60 / 60
60 * 60
763 * 1000 / 60 / 60 * (45)
763 * 1000 / 60 / 60 / 45
763 * 1000 / 60 / 60 / 45 * 4
list_tune_settings <- # Example Estimator evaluation
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 100),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
# Keep only one dataset for every N
neural$datasets <- neural$datasets[c(1, 1001, 2001, 3001)]
mcs_timing <- mcs(dml_estimator, neural)
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 2,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "repeated_cv",
rsmp_args = list(folds = 5, repeats = 3),
par_grids = list_parameterspace,
list_globals = list_globals
))
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 2,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "repeated_cv",
rsmp_args = list(folds = 5, repeats = 3),
par_grids = list_parameterspace,
tune_settings = list_tune_settings,
list_globals = list_globals
))
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 2,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "repeated_cv",
rsmp_args = list(folds = 5, repeats = 3),
par_grids = list_parameterspace,
tune_settings = list_tune_settings,
list_globals = list_globals
))
result
list_estimates
list_tune_settings <- # Example Estimator evaluation
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 100),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Definition Parameter Space.R")
load("Data/Neural.RData")
# Define inputs
vec_ml_g <- c("regr.glmnet", "regr.kknn", "regr.nnet", "regr.ranger",
"regr.rpart", "regr.xgboost")
vec_ml_m <- c("classif.glmnet", "classif.kknn", "classif.nnet", "classif.ranger",
"classif.rpart", "classif.xgboost")
vec_X_cols <- paste0("X.", 1:30)
vec_D_col <- "D"
vec_Y_col <- "Y"
list_tune_settings <- # Example Estimator evaluation
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 100),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
# Keep only one dataset for every N
neural <- neural %>% filter(Samples = 1)
# Keep only one dataset for every N
neural <- neural %>% filter(N = NULL, Samples = 1)
filter.dgp <- function(dgp_obj, N = NULL, Samples = NULL){
# Check that N and Samples are in range
vec_N <- map_dbl(dgp_obj$datasets, ~.x$N) %>% unique()
vec_Samples <- map_dbl(dgp_obj$datasets, ~.x$Sample) %>% unique()
if (!is.null(N) && !any(vec_N %in% N)) {
stop("N is not present in dgp object.", call. = FALSE)
}
if (!is.null(Samples) && !any(vec_Samples %in% Samples)) {
stop("Samples is not present in dgp object.", call. = FALSE)
}
vec_lgl <- dgp_obj$datasets %>% map_lgl(
function(dataset){
if (!is.null(N)) {
lgl_N <- dataset$N %in% N
} else {
lgl_N <- TRUE
}
if (!is.null(Samples)) {
lgl_Samples <- dataset$Sample %in% Samples
} else {
lgl_Samples <- TRUE
}
lgl_N & lgl_Samples
}
)
dgp_obj$datasets <- dgp_obj$datasets[vec_lgl]
dgp_obj
}
# Keep only one dataset for every N
neural <- neural %>% filter(N = NULL, Samples = 1)
mcs_timing <- mcs(dml_estimator, neural)
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 3,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "cv",
rsmp_args = list(folds = 5),
par_grids = list_parameterspace,
tune_settings = list_tune_settings,
list_globals = list_globals
))
library(DoubleML)
library(mlr3verse)
library(tidyverse)
source("Code/DGP class.R")
source("Code/DGP functions.R")
source("Code/Definition Parameter Space.R")
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Simple example to compare scores
load("Data/Sine.RData")
# Keep only N = 100
sine <- sine %>% filter(N = 100)
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 30),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
dataset <- sine$datasets[[1]]$data
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger",
list_globals = list_globals, tune = FALSE
)
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger",
list_globals = list_globals, tune = FALSE
)
"c"
"Code/Definition Parameter Space.R"
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger",
list_globals = list_globals, tune = FALSE, par_grids = list(list_ranger)
)
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger",
list_globals = list_globals, tune = FALSE,
par_grids = list(ranger = list_ranger)
)
subset.dgp <- function(dgp_obj, N = NULL, Samples = NULL){
# Check that N and Samples are in range
vec_N <- map_dbl(dgp_obj$datasets, ~.x$N) %>% unique()
vec_Samples <- map_dbl(dgp_obj$datasets, ~.x$Sample) %>% unique()
if (!is.null(N) && !any(vec_N %in% N)) {
stop("N is not present in dgp object.", call. = FALSE)
}
if (!is.null(Samples) && !any(vec_Samples %in% Samples)) {
stop("Samples is not present in dgp object.", call. = FALSE)
}
vec_lgl <- dgp_obj$datasets %>% map_lgl(
function(dataset){
if (!is.null(N)) {
lgl_N <- dataset$N %in% N
} else {
lgl_N <- TRUE
}
if (!is.null(Samples)) {
lgl_Samples <- dataset$Sample %in% Samples
} else {
lgl_Samples <- TRUE
}
lgl_N & lgl_Samples
}
)
dgp_obj$datasets <- dgp_obj$datasets[vec_lgl]
dgp_obj
}
rm(list = ls())
library(DoubleML)
library(mlr3verse)
library(tidyverse)
source("Code/DGP class.R")
source("Code/DGP functions.R")
source("Code/Definition Parameter Space.R")
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Simple example to compare scores
load("Data/Sine.RData")
# Keep only N = 100
sine <- sine %>% subset(N = 100)
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 30),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
dataset <- sine$datasets[[1]]$data
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
mcs_small <- mcs(dml_estimator, sine)
mcs_small_rob <- run_simulation.mcs(
mcs_obj = mcs_small,
seed = 2, parallel = TRUE, workers = 4, globals = TRUE,
x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = TRUE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger),
list_globals = list_globals
)
library(DoubleML)
install.packages("parallel")
library(mlr3verse)
library(DoubleML)
