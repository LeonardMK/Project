rbind(df_estimates_best)
list_settings_all <- list_settings_all %>%
append(
list(
list_settings
)
names(list_settings_all)[length(list_settings_all)] <- str_name_best
} else {
# Just Rename the elements that are best
str_best_pattern <- paste0("g: ", str_g, " m: ", str_m, "$")
names(list_settings_all)[str_detect(names(list_settings_all), str_best_pattern)] <- str_name_best
}
# Mark for estimates wihich has lowest estimated prediction error
df_estimates <- df_estimates %>%
mutate(
algorithms = case_when(
ml_g == str_g & ml_m == str_m ~ "Best",
TRUE ~ paste0("G: ", str_remove(ml_g, "^.*\\."), " M: ", str_remove(ml_m, "^.*\\."))
)
# Return a list
list(
Estimates = df_estimates,
Settings = list_settings_all,
Measures = df_msrs
)
}
mcs_timing %>% mse()
mcs_timing <- mcs(dml_estimator, neural)
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 1,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = FALSE,
rsmp_key = "repeated_cv",
rsmp_args = list(folds = 5, repeats = 2),
par_grids = list_parameterspace,
list_globals = list_globals
))
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 1,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = FALSE,
rsmp_key = "repeated_cv",
rsmp_args = list(folds = 5, repeats = 3),
par_grids = list_parameterspace,
list_globals = list_globals
))
763 * 1000
763 * 1000 / 60
763 * 1000 / 60 / 60
60 * 60
763 * 1000 / 60 / 60 * (45)
763 * 1000 / 60 / 60 / 45
763 * 1000 / 60 / 60 / 45 * 4
list_tune_settings <- # Example Estimator evaluation
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 100),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
# Keep only one dataset for every N
neural$datasets <- neural$datasets[c(1, 1001, 2001, 3001)]
mcs_timing <- mcs(dml_estimator, neural)
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 2,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "repeated_cv",
rsmp_args = list(folds = 5, repeats = 3),
par_grids = list_parameterspace,
list_globals = list_globals
))
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 2,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "repeated_cv",
rsmp_args = list(folds = 5, repeats = 3),
par_grids = list_parameterspace,
tune_settings = list_tune_settings,
list_globals = list_globals
))
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 2,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "repeated_cv",
rsmp_args = list(folds = 5, repeats = 3),
par_grids = list_parameterspace,
tune_settings = list_tune_settings,
list_globals = list_globals
))
result
list_estimates
list_tune_settings <- # Example Estimator evaluation
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 100),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Definition Parameter Space.R")
load("Data/Neural.RData")
# Define inputs
vec_ml_g <- c("regr.glmnet", "regr.kknn", "regr.nnet", "regr.ranger",
"regr.rpart", "regr.xgboost")
vec_ml_m <- c("classif.glmnet", "classif.kknn", "classif.nnet", "classif.ranger",
"classif.rpart", "classif.xgboost")
vec_X_cols <- paste0("X.", 1:30)
vec_D_col <- "D"
vec_Y_col <- "Y"
list_tune_settings <- # Example Estimator evaluation
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 100),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
# Keep only one dataset for every N
neural <- neural %>% filter(Samples = 1)
# Keep only one dataset for every N
neural <- neural %>% filter(N = NULL, Samples = 1)
filter.dgp <- function(dgp_obj, N = NULL, Samples = NULL){
# Check that N and Samples are in range
vec_N <- map_dbl(dgp_obj$datasets, ~.x$N) %>% unique()
vec_Samples <- map_dbl(dgp_obj$datasets, ~.x$Sample) %>% unique()
if (!is.null(N) && !any(vec_N %in% N)) {
stop("N is not present in dgp object.", call. = FALSE)
}
if (!is.null(Samples) && !any(vec_Samples %in% Samples)) {
stop("Samples is not present in dgp object.", call. = FALSE)
}
vec_lgl <- dgp_obj$datasets %>% map_lgl(
function(dataset){
if (!is.null(N)) {
lgl_N <- dataset$N %in% N
} else {
lgl_N <- TRUE
}
if (!is.null(Samples)) {
lgl_Samples <- dataset$Sample %in% Samples
} else {
lgl_Samples <- TRUE
}
lgl_N & lgl_Samples
}
)
dgp_obj$datasets <- dgp_obj$datasets[vec_lgl]
dgp_obj
}
# Keep only one dataset for every N
neural <- neural %>% filter(N = NULL, Samples = 1)
mcs_timing <- mcs(dml_estimator, neural)
system.time(mcs_timing %>%
run_simulation(
seed = 10,
parallel = FALSE, workers = 3,
x_cols = vec_X_cols,
d_cols = vec_D_col,
y_col = vec_Y_col,
ml_g = vec_ml_g,
ml_m = vec_ml_m,
tune = TRUE,
rsmp_key = "cv",
rsmp_args = list(folds = 5),
par_grids = list_parameterspace,
tune_settings = list_tune_settings,
list_globals = list_globals
))
library(DoubleML)
library(mlr3verse)
library(tidyverse)
source("Code/DGP class.R")
source("Code/DGP functions.R")
source("Code/Definition Parameter Space.R")
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Simple example to compare scores
load("Data/Sine.RData")
# Keep only N = 100
sine <- sine %>% filter(N = 100)
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 30),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
dataset <- sine$datasets[[1]]$data
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger",
list_globals = list_globals, tune = FALSE
)
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger",
list_globals = list_globals, tune = FALSE
)
"c"
"Code/Definition Parameter Space.R"
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger",
list_globals = list_globals, tune = FALSE, par_grids = list(list_ranger)
)
dml_ranger_no_tuning <- dml_estimator(
dataset, x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger", ml_m = "classif.ranger",
list_globals = list_globals, tune = FALSE,
par_grids = list(ranger = list_ranger)
)
subset.dgp <- function(dgp_obj, N = NULL, Samples = NULL){
# Check that N and Samples are in range
vec_N <- map_dbl(dgp_obj$datasets, ~.x$N) %>% unique()
vec_Samples <- map_dbl(dgp_obj$datasets, ~.x$Sample) %>% unique()
if (!is.null(N) && !any(vec_N %in% N)) {
stop("N is not present in dgp object.", call. = FALSE)
}
if (!is.null(Samples) && !any(vec_Samples %in% Samples)) {
stop("Samples is not present in dgp object.", call. = FALSE)
}
vec_lgl <- dgp_obj$datasets %>% map_lgl(
function(dataset){
if (!is.null(N)) {
lgl_N <- dataset$N %in% N
} else {
lgl_N <- TRUE
}
if (!is.null(Samples)) {
lgl_Samples <- dataset$Sample %in% Samples
} else {
lgl_Samples <- TRUE
}
lgl_N & lgl_Samples
}
)
dgp_obj$datasets <- dgp_obj$datasets[vec_lgl]
dgp_obj
}
rm(list = ls())
library(DoubleML)
library(mlr3verse)
library(tidyverse)
source("Code/DGP class.R")
source("Code/DGP functions.R")
source("Code/Definition Parameter Space.R")
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
source("Code/Utils.R")
# Simple example to compare scores
load("Data/Sine.RData")
# Keep only N = 100
sine <- sine %>% subset(N = 100)
list_tune_settings <- list(
terminator = trm("combo",
list(
trm("evals", n_evals = 30),
trm("stagnation", iters = 5)
)
),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
dataset <- sine$datasets[[1]]$data
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
mcs_small <- mcs(dml_estimator, sine)
mcs_small_rob <- run_simulation.mcs(
mcs_obj = mcs_small,
seed = 2, parallel = TRUE, workers = 4, globals = TRUE,
x_cols = paste0("X.", 1:30), y_col = "Y", d_cols = "D",
ml_g = "regr.ranger",
ml_m = "classif.ranger",
draw_sample_splitting = FALSE,
tune = TRUE,
tune_settings = list_tune_settings,
par_grids = list(ranger = list_ranger),
list_globals = list_globals
)
library(DoubleML)
install.packages("parallel")
library(mlr3verse)
library(DoubleML)
library(DoubleML)
install.packages("checkmate")
warnings()
library(DoubleML)
install.packages("R6")
library(DoubleML)
install.packages("devtools")
install.packages("xfun")
library(DoubleML)
library(mlr3verse)
library(tidyverse)
install.packages("tidyverse")
install.packages("make")
R.Version()
install.packages("mlr3")
rm(list = ls())
library(foreign)
library(tidyverse)
df_school <- read.dta("Data/Empirical Application/Data-Files/Data/aer_data_school.dta")
colnames(df_school)
df_pupil <- read.dta("Data/Empirical Application/Data-Files/Data/aer_data_child.dta")
colnmaes(df_pupil)
colnames(df_pupil)
df_school$schoolid %>% unique() %>% length()
rm(list = ls())
install.packages("kknn")
exp(log(0.000001))
library(mlr3verse)
log(1)
log(10)
log(1.1)
log(1.3)
exp(log(0.1))
exp(0.1)
log(exp(0.1))
install.package("gbm")
install.packages("gbm")
install.packages("kknn")
library(DoubleML)
library(mlr3verse)
library(tidyverse)
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
# Specify setup
vec_ml_g <- c("regr.xgboost")
vec_ml_m <- c("classif.xgboost")
vec_X_cols <- paste0("X.", 1:30)
vec_D_col <- "D"
vec_Y_col <- "Y"
list_tune_settings_cv <- list(
terminator = trm("stagnation"),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
list_tune_settings_rcv <- list(
terminator = trm("stagnation"),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("repeated_cv", folds = 5, repeats = 3),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
list_tune_settings_bt <- list(
terminator = trm("stagnation"),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("bootstrap", repeats = 30, ),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
# Check whether estimator works
list_globals = list(
list_design_points = list_design_points,
dml_mean = dml_mean,
calc_err_approx = calc_err_approx,
msr_validation_set = msr_validation_set
)
#
load("Data/Sparse.RData")
install.packages("formula.tools")
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
# Specify setup
vec_ml_g <- c("regr.xgboost")
vec_ml_m <- c("classif.xgboost")
vec_X_cols <- paste0("X.", 1:30)
vec_D_col <- "D"
vec_Y_col <- "Y"
list_tune_settings_cv <- list(
terminator = trm("stagnation"),
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
trms()
trm("evals")
trm_combo <- trm("combo",
list(
trm("evals", 100),
trm("stagnation")
))
trm_combo <- trm("combo",
list(
trm("evals", n_evals = 100),
trm("stagnation")
)
)
list_tune_settings_cv <- list(
terminator = trm_combo,
algorithm = tnr("random_search"),
rsmp_tune = rsmp("cv", folds = 5),
measure = list(ml_g = msr("regr.mse"), ml_m = msr("classif.logloss"))
)
mcs_sparse <- mcs(dml_estimator, sparse)
mcs_sparse <- mcs(dml_estimator, sparse) %>%
subset(Samples = 1:200)
sparse <- sparse %>% subset(Samples = 1:200)
mcs_sparse <- mcs(dml_estimator, sparse)
# Remove sparse to keep working memory ready
rm(sparse)
rm(mcs_sparse)
library(DoubleML)
library(mlr3verse)
library(tidyverse)
rm(list = ls())
source("Code/Estimator Functions.R")
source("Code/Monte Carlo class.R")
source("Code/Monte Carlo Methods.R")
# Specify setup
vec_ml_g <- c("regr.xgboost")
# Detect number of cores
parallel::detectCores()
rm(list = ls())
library(foreign)
library(DoubleML)
library(mlr3verse)
library(tidyverse)
library(haven)
load("Data/Empirical Application/ssaone.RData")
colnames(x)
install.packages("wooldridge")
